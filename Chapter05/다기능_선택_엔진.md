## 📝 Chapter 5: 다기능 선택 엔진

### 1. OpenAI Assistants의 3대 도구 체계 상세 분석

OpenAI는 LLM(거대언어모델)이 텍스트 생성 이외의 작업을 수행할 수 있도록 세 가지 핵심 도구를 제공합니다. 이는 크게 OpenAI 관리형 도구(Hosted)와 개발자 정의형 도구(External)로 나뉩니다.

#### **A. 코드 해석기 (Code Interpreter)**

* **작동 환경**: OpenAI가 관리하는 안전한 샌드박스(Sandbox) 내의 Python 실행 환경.
* **주요 기능**:
* Python 코드 생성 및 즉시 실행.
* 데이터 분석, 시각화(차트 생성), 수학적 계산, 파일 변환.
* **한계**: 인터넷 접속이 불가능하여 외부 라이브러리 설치나 실시간 웹 데이터 접근은 제한됨.
* **호출 방식**: 모델이 필요하다고 판단하면 스스로 Python 코드를 작성하고 실행한 뒤, 그 결과(Logs, Files)를 다시 문맥(Context)으로 가져옴.

#### **B. 파일 검색 (File Search)**

* **기술적 배경**: RAG(Retrieval-Augmented Generation, 검색 증강 생성) 기술의 완전 관리형 버전.
* **작동 원리**:
1. 사용자가 파일을 업로드하면 OpenAI가 이를 청크(Chunk) 단위로 쪼개고 임베딩(Embedding)하여 **벡터 저장소(Vector Store)**에 적재.
2. 질의 시, 의미론적으로 가장 유사한 내용을 검색(Hybrid Search)하여 답변 생성에 활용.


* **특징**: 수천 페이지의 문서에서도 정확한 인용(Citation)과 함께 답변을 제공.

#### **C. 함수 호출 (Function Calling)**

* **정의**: LLM이 외부 API나 자체 데이터베이스와 상호작용할 수 있도록 돕는 기능.
* **핵심**: "함수를 실제로 실행하는 주체는 LLM이 아니다"라는 점이 가장 중요합니다. LLM은 단지 "어떤 함수를 어떤 인자(Argument)로 실행해야 하는지"를 구조화된 데이터(JSON)로 알려줄 뿐입니다.

---

### 2. OpenAI Function Calling의 작동 메커니즘 (The Loop)

OpenAI의 방식은 "클라이언트 주도형 실행(Client-side Execution)"입니다. API는 실행 명령만 내리고, 실제 코드는 개발자의 서버(백엔드)에서 돌아갑니다.

#### **작동 프로세스 (4단계)**

1. **도구 정의 (Definition)**:
* 개발자가 Assistant를 설정할 때, 사용 가능한 함수의 이름, 설명, 파라미터(JSON Schema)를 미리 정의하여 모델에 알려줍니다.
* *예: `get_weather(location: string)*`


2. **모델의 판단 (Thinking & Output)**:
* 사용자가 "서울 날씨 어때?"라고 물으면, 모델은 등록된 도구 중 `get_weather`가 필요함을 인지합니다.
* 모델은 답변 텍스트 대신, **실행 요청 JSON**을 반환합니다.
* *반환값: `{"name": "get_weather", "arguments": "{\"location\": \"Seoul\"}"}*`
* 이때 Assistant의 상태(Status)는 `requires_action`이 됩니다.


3. **실행 (Execution by Client)**:
* OpenAI API로부터 JSON을 받은 개발자의 애플리케이션(클라이언트)이 실제 날씨 API를 호출하거나 DB를 조회하여 결과를 얻습니다.
* *결과: "25도, 맑음"*


4. **결과 제출 및 응답 (Submission)**:
* 개발자는 얻은 결과값("25도, 맑음")을 다시 `submit_tool_outputs` API를 통해 Assistant에게 전달합니다.
* 모델은 이 정보를 바탕으로 최종 자연어 답변을 생성합니다. ("현재 서울은 맑고 25도입니다.")



---

### 3. MCP (Model Context Protocol)의 개념과 구조

MCP는 Anthropic이 주도하는 오픈 표준으로, "모델과 도구 사이의 범용 USB 포트"를 지향합니다.

#### **구조적 특징 (Client-Host-Server)**

* **MCP Server (도구 제공자)**: 실제 기능(예: Google Drive 접근, Slack 메시지 전송, 로컬 DB 조회)을 수행하는 독립적인 프로세스입니다. 특정한 LLM에 종속되지 않습니다.
* **MCP Client (호스트 애플리케이션)**: Claude Desktop, Cursor, 혹은 개발자가 만든 AI 에이전트 등 MCP 프로토콜을 지원하는 프로그램입니다.
* **Protocol (JSON-RPC)**: 클라이언트와 서버가 통신하는 표준 규약입니다.

---

### 4. 핵심 비교: 호출 방식 및 연결 구조의 차이

| 구분 | OpenAI Function Calling | MCP (Model Context Protocol) |
| :--- | :--- | :--- |
| **연결 토폴로지** | **1:1 긴밀한 결합 (Tightly Coupled)**<br>모델(API)과 개발자의 백엔드 코드가 직접 연결됨. | **N:M 느슨한 결합 (Decoupled)**<br>호스트(Client)가 중간에서 모델과 MCP 서버를 중개함. |
| **도구 정의 위치** | **API 요청 본문 (In-Context)**<br>API 호출 시 `tools` 파라미터에 JSON Schema를 직접 포함해서 보냄. | **MCP 서버 내부**<br>서버가 시작될 때 자신이 가진 도구 리스트(`ListTools`)를 호스트에 브로드캐스팅함. |
| **실행 주체** | **개발자의 백엔드 코드**<br>OpenAI가 "이거 실행해줘"라고 JSON을 보내면, 개발자가 작성한 로직(분기문 등)이 해당 함수를 실행함. | **MCP 서버 프로세스**<br>호스트가 JSON-RPC로 요청(`CallTool`)을 보내면, 별도의 MCP 서버가 로직을 수행하고 결과만 반환함. |
| **통신 프로토콜** | **REST API (HTTP)**<br>OpenAI 서버와 개발자 서버 간의 HTTP 요청/응답. | **JSON-RPC (Stdio / SSE)**<br>주로 로컬 프로세스 간 통신(Stdio)이나 웹 소켓(SSE)을 통해 통신. |
| **확장성** | **코드 수정 필요**<br>새 도구를 추가하려면 백엔드 코드를 수정하고 다시 배포해야 함. | **플러그인 방식**<br>새 도구(MCP 서버)를 설치하거나 설정 파일에 추가하면 즉시 인식됨. |


#### **[도해] 호출 흐름의 차이**

**1. OpenAI Function Calling 흐름**


> *특징: "내 앱"이 모든 실행 로직을 품고 있어야 함.*

**2. MCP 호출 흐름**


> *특징: 실행 로직이 "MCP Server"에 격리되어 있어, MCP Client는 도구의 내부 구현을 몰라도 됨.*

---

### 5. 요약 및 결론

* **OpenAI Function Calling**: "나만의 앱"를 만들 때 적합합니다. 내 서비스의 API를 LLM에게 쥐여주고 내 통제하에 작동하게 할 때 가장 강력하고 빠릅니다. 개발자가 실행 과정을 완벽하게 제어합니다.
* **MCP**: "확장 가능한 에이전트 생태계"를 만들 때 적합합니다. 마치 컴퓨터에 프린터 드라이버를 깔듯, 다양한 도구(GitHub, Google Drive 등)를 표준화된 방식으로 연결하고 싶을 때 사용합니다.
